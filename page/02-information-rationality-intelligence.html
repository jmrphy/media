<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>2</title>
  <meta name="description" content="Lecture 2 on the politics of information and intelligence in modernity.">
  <meta name="author" content="Justin Murphy"/>
  <meta name="keywords" content="information, modernity, cybernetics, communication, technology, rationality, alienation, intelligence, media">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <link rel="stylesheet" type="text/css" href="http://jmrphy.net/media/css/tufte.css">
  <!-- <link rel="stylesheet" type="text/css" href="http://jmrphy.net/media/css/print.css" media="print"> -->

  <link rel="canonical" href="http://jmrphy.net/media/page/02-information-rationality-intelligence">

  <link rel="alternate" type="application/rss+xml" title="Politics of the Media" href="http://jmrphy.net/media/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
	<!--- <a href="http://jmrphy.net/media/"><img class="badge" src="http://jmrphy.net/media/assets/img/badge_1.png" alt="CH"></a> -->
	
		
		    
		      <a href="http://jmrphy.net/media/page/01-politics-of-media-overview-main-themes">1</a>
		    
	    
  	
		
		    
		      <a class="active" href="http://jmrphy.net/media/page/02-information-rationality-intelligence" class="active">2</a>
		    
	    
  	
		
		    
		      <a href="http://jmrphy.net/media/page/03-modernity-and-acceleration">3</a>
		    
	    
  	
		
		    
		      <a href="http://jmrphy.net/media/page/04">4</a>
		    
	    
  	
		
		    
		      <a href="http://jmrphy.net/media/page/05">5</a>
		    
	    
  	
		
		    
		      <a href="http://jmrphy.net/media/page/06">6</a>
		    
	    
  	
		
		    
		      <a href="http://jmrphy.net/media/page/07">7</a>
		    
	    
  	
		
		    
		      <a href="http://jmrphy.net/media/page/08">8</a>
		    
	    
  	
		
		    
		      <a href="http://jmrphy.net/media/page/09">9</a>
		    
	    
  	
		
		    
		      <a href="http://jmrphy.net/media/page/10">10</a>
		    
	    
  	
		
		    
		      <a href="http://jmrphy.net/media/page/11">11</a>
		    
	    
  	
		
		    
		      <a href="http://jmrphy.net/media/page/12">12</a>
		    
	    
  	
		
  	
		
		    
		      <a href="http://jmrphy.net/media/">About</a>
		    
	    
  	
		
		    
		      <a href="http://jmrphy.net/media/css/print.css"></a>
		    
	    
  	
		
  	
	</nav>
</header>
    <article class="group">
      <h1>2</h1>
<p class="subtitle"></p>


<!--   What is information & communication & why it matters -->

<!--   Information & communication -->

<p>Today we take for granted such platitudes as, “the world is made of information,” but only a few decades ago such a phrase would make very little sense to anyone. Even today, most people who repeat such commonplaces really could not tell you very much about what exactly information is. We talk to each other everyday and assume we are communicating, but are we really?</p>

<p>The precise nature of information is a very recent discovery. Arguably it is one of the most crucial discoveries that would lead to what we now call the Information Age. The nature of information was formalized in 1948 by Claude Shannon, working at Bell Labs. It is an alluring story that has already been told better than I could tell it here, although we should try to better understand the political implications.<label for="sn-id-gleick" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-id-gleick" class="margin-toggle" /><span class="sidenote">For an accessible, enjoyable, and rigorous read, I recommend Gleick, James. 2011. The Information: A History, a Theory, a Flood. London: Harper Collins. </span></p>

<p>Probably the best, short, intuitive definition of information was written by Gregory Bateson: “a difference which makes a difference.” If I tell you something you already know, I’m not giving you information; that would be a difference that does not make a difference. If I tell you something you do not already know, I would be giving you information. Your world would change, ever so slightly.</p>

<p>But any message or signal can have more or less information content. David Krakauer of the Santa Fe Institute provides the following example to help build your intuition of this idea. If you want to find a specific location–say, my flat in Southampton–you could just drive all over Southampton randomly until you find it. That would take a lot of time, with a lot of error, before you eventually found it. But what if I drew you a map from where you are now, at the university, to my flat, with arrows along the streets you should take and a circle around where my flat is located? The information content of the map can be measured by <em>the amount of time the map saved you.</em><label for="sn-id-krakauer-info" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-id-krakauer-info" class="margin-toggle" /><span class="sidenote">Harris, Sam. 2016. “Complexity &amp; Stupidity: A Conversation with David Krakauer.” Waking Up (Podcast). https://www.samharris.org/podcast/item/complexity-stupidity/ (October 2, 2017). </span> Information is basically the reduction of uncertainty.</p>

<p>Claude Shannon mathematically formalized this basic intution, and laid the groundwork for any number of technological advancements based on the increasingly sophisticated command of information. Shannon’s mathematical theory of information is way above my pay grade, and for our purposes there is no need to grapple with any of the mathematics, but it is necessary to build up an intuitive sense of this new way to think about information and communication. We should also try to gain a sense of the political context and the political, historical consequences that flow from this model of information and communication.</p>

<h2 id="bell-labs-and-the-information-revolution">Bell Labs and the Information Revolution</h2>

<p>Bell Labs had its origin in the research laboratory setup by Alexander Graham Bell in 1880 with money he was awarded by the French government for inventing the telephone. The lab was dedicated to the study of sound processing. The modern institution known as Bell Labs was founded in 1925 in the merging of Western Electric’s research department and the engineering department of the American Telephone &amp; Telegraph company (AT&amp;T). Its original main focus was to improve the commercial operation of telephone exchange switches, but with an open-ended agenda for extending the frontiers of human knowledge around information processing in general. They also worked for the US government on commission, such as in Project Nike (1945) to develop anti-aircraft technology and the Apollo Program (1961) which would put the first humans on the moon, but they also did pure scientific research at the forefront of the mathematical sciences. Seven Nobel Prizes in Physics were awarded to Bell Labs Researchers between 1937 and 2009.</p>

<p>The achievements within Bell Labs throughout the twentieth century were extraordinary. Perhaps the largest and most well-funded pursuit of scientific knowledge ever mobilized under one organizational umbrella–driven explicitly by the pursuit of profit and then in cooperation with the interests of state power–had a significant role in almost every technological advancement that marked the twentieth century. In the 1920s, Bell Labs was responsible for the first public demonstration of the fax machine, the first motion picture with sound, the first long-distance transmission of television images. Behind these now well-known consumer technologies, however, were the formal mathematical advancements of which these technologies were only applications. In particular, the mathematical advancements all had to do with the nature of information. Thus, it was also in the 1920s that Bell Labs pioneered the essential concepts of what is now known as “statistical process control,” the mathematical foundations of measuring the stability and efficiency of processes (of an assembly line, for instance) and designed the first ever technically unbreakable cipher.</p>

<p>In 1947, Bell Labs researchers John Bardeen, Walter Brattain invented the transistor, arguably the most important advancement in twentieth-century electronics. William Shockley, also of Bell Labs, is the figure most directly responsible for the commercialization of the transistor. His Shockley Semiconductor Laboratory, established in Mountain View, California, was the epicenter of what would later become known as Silicon Valley. Although his commercial efforts largely failed, several of Shockley’s employees branched out and started more than 60 new enterprises in the same part of California. These enterprises included such names as Intel and ADM. Interestingly, Shockley was also an outspoken racist who believed in eugenics.</p>

<p>It was in 1948 that the Bell Labs Technical Journal published Claude Shannon’s “A Mathematical Theory of Communication”, the founding document of what would come to be known as information theory.</p>

<p>Shannon’s piece is so crucial because it states more exactly than ever before the essential mathematical structure of communication. As Shannon points out, the essence of communication is simply the process of transmitting information from one point to another point. But the defining problem which communication responds to is the fact that the world is composed of “noise,” a variable but always-present background of criss-crossing signals through which purposeful communication has to pass. Go into a silent room and notice that if you listen closely you can always hear a soft hum coming from the world, if only the tiniest vibrations of air in your ear. That’s noise, but it’s relativley little noise, that’s why it’s easy to communicate with someone in such a silent room. If you’re at a music concert and a band is playing, the noise might be so loud that you cannot communicate to anyone at all: this would mean there is so much noise that the signals you’re sending never make it into the other person’s ear. The reason your friend can’t understand you is because your signal is scrambled by the large quantity of other signals in the background.</p>

<p>The formal terms for this essential structure are as follows. An information source produces a message. A transmitter operates on the message to generate a signal. A signal is sent through a channel (with some variable amount of noise). A receiver receives the message and transforms the signal back into a message. Finally, the message arrives at a destination.</p>

<figure><figcaption>Shannon, Claude E. 1948. “A Mathematical Theory of Communication.” The Bell System Technical Journal 27(3): 379–423, 623–656. Page 7.</figcaption><img src="http://jmrphy.net/media/assets/img/Shannon-communication-7.png" /></figure>

<p>This simple model served as the basis for an extremely sophisticated mathematical development of the nature of communication. The mathematical developments supercharged the rigor and efficiency of a wide variety of real-world endeavours, unsurprisingly centered around maximizing the profits, power, and control of those who put these advancements into practice (indeed “control theory” becomes the literal name of one branch of information theory).</p>

<p>Following Tukey, who used the word in a 1947 Bell Labs internal memo, Shannon deployed the concept of “bit” as the basic unit of information. A bit is simply the amount of information gained when the value of a binary random value (taking the value of either 0 or 1) becomes known. So if there is a 50% chance a coin will land heads rather than tails, and after a flip it indeed lands heads, one bit of information is gleaned. Shannon is commonly considered the father of the digital revolution because his formalization of the bit as the basic unit of information allowed for more efficient communication, quicker and less noisy than analog.</p>

<h2 id="entropy-and-intelligence">Entropy and intelligence</h2>

<p>Information, in short, reduces randomness or chaos. A technical term for randomness or chaos is <em>entropy.</em> Roughly speaking, entropy is the opposite of information. In any closed system, entropy increases over time, a point that follows from the Second Law of Thermodynamics (Eddington 1928). It’s interesting to note that time’s arrow, our sense that time “moves forward,” is only due to entropy. Time seems to move “forward” only because time brings randomness or chaos that breaks things in ways we cannot put back together with full fidelity. If you drop a glass and it shatters into a thousand pieces, you cannot undrop the glass and make it fall upward reuniting into its original form. If you could do this, for everything, you would have no reason to feel or believe that time moves forward. If everything could be reversed with full fidelity, all time would feel–and for all intents and purposes <em>be</em>–reversible. It should also be noted that in an open system, local entropy can decrease over time, but at the cost of increasing global entropy. An example is whenever you create any kind of order, as in cleaning your room or constructing a building: you are increasing the information, and decreasing the entropy, <em>in your room or in the building</em>, but only by burning calories and emitting heat into the global system.</p>

<p>The concept of intelligence is tightly related to the concepts of information and entropy. One commonly cited definition of intelligence is that it measures an agent’s ability to achieve goals in a wide range of environments.<label for="sn-id-legg-hutter" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-id-legg-hutter" class="margin-toggle" /><span class="sidenote">Legg, Shane, and Marcus Hutter. 2007. “Universal Intelligence: A Definition of Machine Intelligence.” Minds and Machines 17(4): 391–444. Page 12. </span></p>

<p>Consider the example of a rubik’s cube. If you spin all the slices randomly, you’ll eventually solve it, though you’d probably die first. If you only spin one slice of it over and over again, that’s called stupidity: you’ll never solve it, so that’s worse than spinning all the slices randomly. Intelligence involves identifying and following the rules that lead to the quickest solution. Just as the information content of a map can be quantified by the time it saves you to find something, the intelligence of an agent can be indexed by its time advantage relative to randomness or stupidity.</p>

<p>Now, a crucial point is that intelligence can co-exist with any number of final goals. This is what Bostrom calls the “orthogonality thesis,” that any level of intelligence can co-exist with any final goal. Another way to put this, or another implication of this, which seems quite important to me, is that intelligence, strictly speaking, cannot select final goals. As intelligence is only measured relative to an already given final goal, intelligence must take its ultimate orders from something or somewhere other than intelligence.</p>

<h2 id="diagram-of-intelligence">Diagram of intelligence</h2>

<figure><figcaption>Legg, Shane, and Marcus Hutter. 2007. “Universal Intelligence: A Definition of Machine Intelligence.” Minds and Machines 17(4): 391–444. Page 16.</figcaption><img src="http://jmrphy.net/media/assets/img/Legg-Hutter-Intelligence-16.png" /></figure>

<p>As the diagram from Legg and Hutter shows, intelligence has a kind of circular structure. It is an inherently dynamic process in which some actor or control center is in two-way communication with an outside environment. Somewhat loosely we might say that when this two-way transmission circuit is characterized by low levels of noise, intelligence is greater than when this circuit is characterized by high levels of noise.</p>

<p>IQ is a measure of intelligence in human beings. In the psychology literature, human intelligence is often denoted as <em>g</em> or the <em>general cognitive factor</em>. IQ is a measure of <em>g</em>. It is one of the best measured and best understood of human traits. IQ is one of the single best predictors of success in many domains (such as educational performance, career success, income, even longevity). Some scholars believe that IQ is inversely related to genetic mutational load. Recall that random genetic mutations are useful for a species, as new and unpredictable environmental challenges might be dealt with by one of many random mutations; while everyone possessing other random mutations will be slightly disadvantaged, the useful mutation will be selected for, it will spread, and the other mutations will not. So many random genetic mututations are good for a species, but any particular mutation is generally not good for any particular individual. So accumulated genetic mutations appear negatively correlated with a human’s general problem-solving power, and the greatest problem-solving power in humans is perhaps a result of very low mutational load.</p>

<p>So intelligence produces extropy (also known as negentropy), i.e. local entropy reduction, or “what it means for something to work.” Finally, yet another way to think about it is to say that intelligence or extropy leads to the non-ocurrence of probable outcomes (the infinite number of ways a building can collapse), and the occurrence of improbable outcomes (e.g., the small number of ways a building can stand).</p>

<h2 id="human-intelligence-and-ideology">Human intelligence and ideology</h2>

<p>Human intelligence is one of the most well understood human traits. It varies significantly across humans. It is partially genetic. What does it mean that human beings have different problem-solving power? Following from above, one implication is that humans live on different time scales. In some non-trivial sense, high-IQ people are living farther in the future than low-IQ people. This is why speaking with someone significantly smarter than you is quite literally speaking with an alien who has come from the future. And speaking with someone less smart than you is like speaking with someone from the past. You can think about this even in a minute-by-minute sense. As a highly intelligent person speaks, a less intelligent person is still inhabiting the first words of the sentence while the speaker is finishing the last words, whereas an even more intelligent listener is correctly predicting a speaker’s second sentence before they are even finished with their first sentence.</p>

<p>But we all live together, so society is basically a kind of inter-temporal zone. With the rise of mass media, the reality that receives sponsorship will be a weighted average of the audience’s time-slice. Smarter people will always be ahead, dumber people will always be behind, but most of the people with average intelligence will converge on a quotidian reality, all the more reinforced by a shared, external, socio-political legitimation. One of the key problems with a mass-media culture in a capitalist society is that any ideas overly contradicting or offending mass sensibilities will be selected out of the mass-media model of reality, as a simple outcome of supply and demand. Unsurprisingly, one of the key planks of contemporary ideology is that there do not exist significant, genetic psychological and behavioral differences between individuals and groups.</p>

<p>Marx thought that ideology protected the status quo by reflecting the interests of the bourgeoisie, and it does, but not by giving the bourgeoisie disproportionate power to project their ideas. Ideology operates by promoting the stupid ideas, especially those preferred by the weak and the poor. This is how left-wing compassion can be a kind of vicious commitment to the weak and poor remaining weak and poor. By treating less intelligent people as equally intelligent, you are actively conspiring to ensure your advantage remains stable. With ideology, it is not that the ideas of the ruling class are favored, but the opposite: the stupid ideas of the stupid are respected, for the economic gains of the smart. Nowhere do you see this more clearly than with a company such as Google, which will fire an employee for writing about psychological differences between the sexes, while actively leveraging the reality of those differences in advertising market segmentation.</p>

<p>So the social taboo on human psychological differences is itself a piece of stupidity, that ensures subjects continue to follow routines that do not work, like trying to solve a rubik’s cube by spinning one slice over and over. On the other hand, the unfettered play of intelligenic differences is no doubt catastrophic. That is what’s happening today, in the rise of inequality and mass cultural confusion. The problem is that intelligence is precisely that which escapes constraints designed by the less intelligent. Tax evasion, for instance. We cannot tax the rich even if we wanted to, they are too smart relative to bureacratic governmental institutions.</p>

<h2 id="cybernetics">Cybernetics</h2>

<p>The general science of extropy production is cybernetics. Intelligence is intrinsically a dynamic, cybernetic process.</p>

<p>But cybernetic systems can evolve, and a local cybenertic system can enter into a more complex cybernetic system. The thermostat in your house prevents the temperature from going to zero (maintaining equilibrium locally or within the house, maintaing disequilibrium relative to the outside of the house). But this one local cybernetic system of negative feedback within the house multiplies the amount of work you can do throughout the winter, which might involve, for instance, designing many more cybenertic systems, making you more productive, and so on. So one process of negative feedback, sustaining equilibrium within the house.</p>

<p>The superintelligence of an AI system such as capitalism is the product of escalating cybernetic systems, themselves in positive feedback, emerging from organic life. So contemporary global capitalism is an incredibly improbable situation, it organizes extraordinary amounts of information, because it is an evolved intelligent system. It is a cybernetic system of cybernetic systems. And it continues to be a self-increasing in its intelligence, information, or improbability.</p>

<h2 id="modernity-as-means-ends-reversal">Modernity as means-ends reversal</h2>

<p>One way to define modernity is as a means-ends reversal. Roughly speaking, the premodern human chooses means to achieve some chosen end. For most of human history, this means the development of ways to increase the probability of survival through practical rationality and marginal, piecemeal technological innovation. With modernity, the instrumental goal of increasing productive capacity through innovation (instrumental for the larger goal of survival), becomes the primary goal itself. The innovations most instrumental for the securing of human survival (as evidenced by population explosion) can be summarized with the word “capitalism.” Exactly how to date this turning point, and how to define it’s components precisely, is a fool’s errand. It suffices to say that mathematical sophistication, technological discoveries (printing press), later steam engine, etc.), and institutional innovations (double-entry bookkeeping, joint stock companies, etc.) conspired around the 1600s (+/- 200 years) to ignite a systemic change in human society of incomparable historical significance. All of these events, which increased the power of our <em>means</em> to achieve <em>ends</em> of human interest, takeoff at lightspeed such that, soon, they become the end of human life. The cultivation of means (stored, potential capacity) is the only end. In modernity, we no longer work and innovate to promote our own purposes; we trade our toil to a superintelligent system, which accumulates sheer potentiality, in exchange for our survival. Securing the existence of capitalism into the future is now the ultimate purpose of human life today, whether we like it or not (at least if you wish to live, which is why many anti-capitalists do not wish to live). If you are trying to live today, you are working for the reproduction of capitalism. You can say this is unjust–it most certainly is–and you are welcome to die if you do not wish to accept the bargain you are being offered. You are also welcome to outsmart capitalism, if you can, but the whole problem is that we are talking about a superintelligent system. Lefty activists who role-play as if they are outsmarting capitalism are not even really trying to do so; typically they are adopting a coping strategy to sustain their survival under a superintelligence the complexity of which they often cannot fathom.</p>

<h2 id="capitalism-as-artifical-superintelligence">Capitalism as artifical superintelligence</h2>

<p>Articial intelligence and capitalism are the same thing: the tool escaping from it’s subordination to a human purpose, to becoming purpose in its own right. It’s absolutely unjust and horrifying, so it’s appropriate to be “opposed” to this, but the problem is that any effort to outsmart it, feeds it. Because “intelligence optimization and means-end reversal are the same thing.” You can oppose the means-end reversal, but that is quite literally the promotion of stupidity. This is why activist today is a combination of self-punishing work regimines (trying to outsmart a superintelligent system) and actively disincentiving intelligence. Activist groups today cannot avoid being alienating and oppressive work forces (you have to go to meetings, and pay your dues, and plan events, etc.), all investment, all cultivation of means for a final purpose or end that never, ever comes (the better society, liberation, etc.). And they must place increasingly tight restrictions on the expression of intelligence, both in their groups and in society at large. For intelligence and capitalism are in cahoots, that is one key reasons for its bewildering capacity to channel human energies despite extraordinary human resistance throughout history. I’m an anticapitalist to whatever degree this notion can be defined coherently, but one cannot conscionably be anti-intelligence, for that is about as close as you can come to the definition of wilfull evil. I would rather be honest and clear that I am hypocrite, while at least revealing a hostage situation for what it is, than feign a righteous coherence that capitalism has objectively banished, in order to pretend I am not in a hostage situation. My hunch is that this is the path to any genuine and serious politics today, but more on this in later sections.</p>


    </article>
    <span class="print-footer">2 - Justin Murphy</span>
    <footer>
  <hr class="slender">
  <ul class="footer-links">   
    
      <li>
        <a href="//jmrphy.tumblr.com/ask"><span class="icon-mail"></span></a>
      </li>
    
      <li>
        <a href="//www.twitter.com/jmrphy"><span class="icon-twitter"></span></a>
      </li>
    
      <li>
        <a href="//github.com/jmrphy"><span class="icon-github"></span></a>
      </li>
    
      <li>
        <a href="/feed"><span class="icon-feed"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>&copy; 2017 &nbsp;&nbsp;JUSTIN MURPHY</span></br> <br>
<span>This site created with the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme</a> in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>

	<script src="https://my.hellobar.com/4fa9f552f76ae17579c82c1d40803e06738e0390.js" type="text/javascript" charset="utf-8" async="async"></script>

  </body>
</html>
